# -*- coding: utf-8 -*-
"""Learning Analytics - Tree Regression /Hyper Parameter Tuning_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1v8yvDmHJeOKDKETEX-pUuzJ24jgPjxl7

# **Data Preperation**
"""

import numpy as np
import requests
import json

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns #Vizualisation
import matplotlib.pyplot as plt #Vizualisation

from sklearn.model_selection import train_test_split

# Load data
data = []
url='https://la-api.codeiin.com/students/behaviour'
r = requests.get(url)

# Load data
data = []
url1='https://la-api.codeiin.com/users/get'
usersData = requests.get(url1)

json_dict = r.json()  
json_dict1 = usersData.json()

print(type(json_dict))
JSONContent= json.dumps(json_dict)
JSONContent1= json.dumps(json_dict1)
JSONContent
JSONContent1

# Convert JSON string with records orient to a Dataframe
df = pd.read_json(JSONContent, orient ='columns')
df1 = pd.read_json(JSONContent1, orient ='columns')

# Drop non-necessary futures of the data id-column
df = df.drop("_id", axis=1)
df
df1 = df1.drop(["_id","email","password"], axis=1)
df

#Overview of the features and label 
df

#check if there are any null values in our data
df.isnull().sum()

import seaborn as sns
sns.heatmap(df.isnull(),yticklabels=False)

#Category features
for cat in df.columns:
  print(cat)

#OneHotEncoder-Function
def OneHotEncoder (df_nominal):
  oneHotEncodedData = pd.get_dummies(df_nominal, prefix_sep = "_")
  return oneHotEncodedData

#Categorical-Values
categorical_columns=['Certification Course','Gender','Department','hobbies','prefer to study in','Do you like your degree?','part-time job']

#Dataframe with only categorical values
categorical = df[categorical_columns]

#OneHotEncoder-Function on nominal features
nominal=categorical
df_nominal = OneHotEncoder(nominal)
df_nominal

"""# **ORDINAL DATA - ORDINAL ENCODER**

Due to the lack of functionalietes of the OneHotEncoder of we did the ordinal encoding with the help of dictionaries to be sure, every feature is ordered in the right way.
"""

#OrdinalEncoding
df_ordinal = df[['daily studing time',
       'willingness to pursue a career based on their degree  ',
       'social medai & video', 'Travelling Time ', 'Stress Level ',
       'Financial Status']]

# leveraging the variables already created above
mapper = {'0 - 30 minute':1, '30 - 60 minute':2, '1 - 2 Hour':3, '2 - 3 hour':4,
       '3 - 4 hour':5, 'More Than 4 hour':6}
df_ordinal['daily studing time'] = df_ordinal['daily studing time'].replace({'0 - 30 minute':1, '30 - 60 minute':2, '1 - 2 Hour':3, '2 - 3 hour':4,
       '3 - 4 hour':5, 'More Than 4 hour':6})

mapper = {'50%':3, '75%':4, '25%':2, '100%':5, '0%':1}
df_ordinal[ 'willingness to pursue a career based on their degree  '] = df_ordinal[ 'willingness to pursue a career based on their degree  '].replace({'50%':3, '75%':4, '25%':2, '100%':5, '0%':1})

mapper = {'1.30 - 2 hour':2, '1 - 1.30 hour':3, 'More than 2 hour':1, '30 - 60 Minute':4, '1 - 30 Minute': 5, '0 Minute': 6}
df_ordinal['social medai & video'] = df_ordinal['social medai & video'].replace({'1.30 - 2 hour':2, '1 - 1.30 hour':3, 'More than 2 hour':1, '30 - 60 Minute':4, '1 - 30 Minute': 5, '0 Minute': 6})

mapper = {'30 - 60 minutes':6, '0 - 30 minutes':7, '1 - 1.30 hour':5,
       '2 - 2.30 hour':3, '1.30 - 2 hour':4, 'more than 3 hour':1,
       '2.30 - 3 hour':2}
df_ordinal['Travelling Time '] = df_ordinal['Travelling Time '].replace({'30 - 60 minutes':6, '0 - 30 minutes':7, '1 - 1.30 hour':5,
       '2 - 2.30 hour':3, '1.30 - 2 hour':4, 'more than 3 hour':1,
       '2.30 - 3 hour':2})

mapper = {'Bad':2, 'Awful':1, 'Good':3, 'fabulous':4}
df_ordinal["Stress Level "] = df_ordinal["Stress Level "].replace({'Bad':2, 'Awful':1, 'Good':3, 'fabulous':4})

mapper = {'Bad':2, 'good':3, 'Awful':1, 'Fabulous':4}
df_ordinal["Financial Status"] = df_ordinal["Financial Status"].replace({'Bad':2, 'good':3, 'Awful':1, 'Fabulous':4})

#Dataframe with ordinal encoded features instead of Strings
df_ordinal

#Overview about data of the ordinal features
df_ordinal.describe()

"""# ***Label Encoding***"""

label = df["college mark"]

numerical = df[['10th Mark', '12th Mark']]
df = pd.concat([df_ordinal, df_nominal,numerical, label], axis = 1)

df.columns

df_feature_selection = df[['10th Mark', '12th Mark',
      'prefer to study in_Anytime', 'daily studing time',
      'social medai & video' , 'prefer to study in_Morning','prefer to study in_Night', 'Department_B.com ISM','hobbies_Cinema','Travelling Time ',
      'Gender_Female', 'Financial Status', 'college mark']]

df_feature_selection.dtypes

df=df_feature_selection
newDF = df.loc[:,~df.T.duplicated(keep='first')]

df1.rename(columns={"10th Mark": "tenthMark" ,  "12th Mark": "twelfthMark" ,"prefer to study in_Anytime": "studyAnytime" ,"prefer to study in_Night": "studyNight","prefer to study in_Morning": "studyMorning",
                    "hobbies_Cinema" : "hobbiesCinema" , "hobbies_Sports": "hobbiesSports","daily studing time":"dailystudingtime", "social medai & video": "socialMedia",
                     "Stress Level ": "stressLevel", "willingness to pursue a career based on their degree  ": "pursueCareer" ,
                      "Department_B.com ISM": "Department_B_ISM" , "Travelling Time " : "travellingTime",
                       "Gender_Female":"gender" ,"financialStatus":"Financial Status" }, inplace = True)

frames = [df, df1]
newDF = pd.concat(frames)
newDF = df
newDF

relation=df.corr()
relation_index=relation.index
relation_index

sns.heatmap(df[relation_index].corr(),annot=True)

"""# **`Train-Test-Split`**"""

# Feature selection by ExtraTreesRegressor(model based)

from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score as acc
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import LabelEncoder   


#Train Test Split - Drop less important features - tbd
y = df['college mark']

X = df.drop(['college mark'],axis = 1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train

"""# **Training Phase 1: Outlier Detection**
First of all we want to identify outliers and train the model, to see the results of the model as the baseline for optimizations. As models for testing we are using Decision Tree Regressor and Linear Regression model.

To identify outliers we use the LocalOutlierFactor based on sklearn.neighbors.

"""

#create Dataframe to work on
newDF = df

from sklearn.neighbors import LocalOutlierFactor
from sklearn.tree import DecisionTreeRegressor


# Create the LOF object
lof = LocalOutlierFactor(n_neighbors=20, contamination=0.1)

# Fit the LOF model to the dataset
y_pred = lof.fit_predict(newDF)


# Identify the outliers

# outliers = list(np.where(y_pred == -1)[0])
# outliers = [int(i) for i in outliers]

outliers = list(np.where(y_pred == -1)[0].tolist())


#Train Test Split - Drop less important features - tbd
y = newDF['college mark']

X = newDF.drop(['college mark'],axis = 1)

X = X.astype(np.int32)

print('outliers = ' , outliers)

# Remove outliers from the dataset
X_clean = X[np.isin(np.arange(X.shape[0]), outliers, invert=True)]

# Remove the corresponding target values
y_clean = y[np.isin(np.arange(y.shape[0]), outliers, invert=True)]
# y_clean = np.delete(y, outliers, axis=0)


X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.3, random_state=42)

# Create the decision tree model
#regressor = DecisionTreeRegressor(random_state=0)

# Fit the model to the cleaned data
#regressor.fit(X_train, y_train)

print(len(X_train), len(X_test), len(y_train), len(y_test) ,len(X))

#Decision Tree Model Training - Results
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
model.fit(X_train,y_train)

print(model.score(X_train,y_train))
print(model.score(X_test,y_test))

#Displot of predicted scores
y_predicted = model.predict(X_test)
sns.distplot(y_predicted)

#Comparison between label, predicted and original
d = {'scores': y_predicted, 'original': y_test}
scoreTable = pd.DataFrame(data=d)
scoreTable

#Linear Regression Model - results
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train,y_train)

print(lr.score(X_train,y_train))
print(lr.score(X_test,y_test))

y_predicted = lr.predict(X_test)
sns.distplot(y_predicted)

#Comparison between predicted and original scores
d = {'scores': y_predicted, 'original': y_test}
scoreTable = pd.DataFrame(data=d)
scoreTable

"""# **Training Phase 2: Feature Selection**

To select features we use the feature importances of the different features which can be calculated by all kinds of Regression Tree Models.
We make use of the Extra Tree Regressor Model.

"""

#Training of the model to calculate feature importances
model= ExtraTreesRegressor()
model.fit(X_train, y_train)

#Output of feature importances
model.feature_importances_

#Bar plot of feature importances to get an overview of results
feat_importances = pd.Series(model.feature_importances_, index=X_train.columns)
feat_importances.nlargest(20).plot(kind='barh')
plt.show()

#20 largest features 
feat_importances.nlargest(40).index

features = ['12th Mark', '10th Mark', 'prefer to study in_Night',
       'Financial Status', 'daily studing time',
       'willingness to pursue a career based on their degree  ',
       'Stress Level ', 'Travelling Time ', 'social medai & video',
       'Gender_Female', 'hobbies_Cinema', 'prefer to study in_Anytime', 'prefer to study in_Morning',
      'hobbies_Sports', 'Department_B.com ISM']

#Reduce Training-Data to most important features
X_train = X_train[features]
X_train.shape

X_test = X_test[features]
X_test.shape

#Reduce Training-Data to most important features
X_train_features = X_train[features]
X_test_features = X_train[features]

# X_train
# X_train.rename(columns={"10th Mark": "tenthMark" ,  "12th Mark": "twelfthMark" ,"prefer to study in_Anytime": "studyAnytime" ,"prefer to study in_Night": "studyNight","prefer to study in_Morning": "studyMorning",
#                     "hobbies_Cinema" : "hobbiesCinema" , "hobbies_Sports": "hobbiesSports","daily studing time":"dailystudingtime", "social medai & video": "socialMedia",
#                      "Stress Level ": "stressLevel", "willingness to pursue a career based on their degree  ": "pursueCareer" ,
#                       "Department_B.com ISM": "Department_B_ISM" , "Travelling Time " : "travellingTime",
#                        "Gender_Female":"gender" ,"financialStatus":"Financial Status"}, inplace = True)

# X_train

X_train.shape

X_train

y_train.rename("finalGrade", inplace = True)

y_train.shape

# X_test.rename(columns={"10th Mark": "tenthMark" ,  "12th Mark": "twelfthMark" ,"prefer to study in_Anytime": "studyAnytime" ,"prefer to study in_Night": "studyNight","prefer to study in_Morning": "studyMorning",
#                     "hobbies_Cinema" : "hobbiesCinema" , "hobbies_Sports": "hobbiesSports","daily studing time":"dailystudingtime", "social medai & video": "socialMedia",
#                      "Stress Level ": "stressLevel", "willingness to pursue a career based on their degree  ": "pursueCareer" ,
#                       "Department_B.com ISM": "Department_B_ISM" , "Travelling Time " : "travellingTime",
#                        "Gender_Female":"gender" ,"financialStatus":"Financial Status"}, inplace = True)

# X_test

X_test.shape

y_test.rename("finalGrade", inplace = True)

#Decision Tree Model - Re-Training with more specific features
from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
model.fit(X_train,y_train)

print(model.score(X_train,y_train))
print(model.score(X_test,y_test))

#Displot of results of the model
print(model.score(X_train,y_train))
print(model.score(X_test,y_test))
y_predicted = model.predict(X_test)
sns.distplot(y_predicted)

#Comparison of the predicted and original scores
d = {'scores': y_predicted, 'original': y_test}
scoreTable = pd.DataFrame(data=d)
scoreTable

#Error rate without hyperparameter tuning 
from sklearn import metrics

print(len(y_test),len(y_pred))

print( 'MSE : ' , metrics.mean_squared_error(y_test,y_predicted))
print( 'MAE : ' , metrics.mean_absolute_error(y_test,y_predicted))
print( 'RMAE : ' ,np.sqrt(metrics.mean_squared_error(y_test,y_predicted)))

#Linear Regression Model - Training
from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train,y_train)

X_train

X_test

print(lr.score(X_train,y_train))
print(lr.score(X_test,y_test))
y_predicted = lr.predict(X_test)
sns.distplot(y_predicted)

d = {'scores': y_predicted, 'original': y_test}
scoreTable = pd.DataFrame(data=d)
scoreTable

#Error rate without hyperparameter tuning 
from sklearn import metrics

print(len(y_test),len(y_pred))

print( 'MSE : ' , metrics.mean_squared_error(y_test,y_predicted))
print( 'MAE : ' , metrics.mean_absolute_error(y_test,y_predicted))
print( 'RMAE : ' ,np.sqrt(metrics.mean_squared_error(y_test,y_predicted)))

"""# **Training Phase 3 - Model Selection**

Before we introduce hyper parameter tuning we want to choose the best model to solve the regression task. To do that we compare all kinds of Regression models.


"""

from time import time
from sklearn.linear_model import LinearRegression, Ridge,Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor

from sklearn.metrics import explained_variance_score,mean_absolute_error,r2_score

regressors = [
    KNeighborsRegressor(),
    GradientBoostingRegressor(),
    KNeighborsRegressor(),
    ExtraTreesRegressor(),
    RandomForestRegressor(),
    DecisionTreeRegressor(),
    LinearRegression(),
    Lasso(),
    Ridge()
]

head = 10
for model in regressors[:head]:
    start = time()
    model.fit(X_train, y_train)
    train_time = time() - start
    start = time()
    y_pred = model.predict(X_test)
    predict_time = time()-start    
    print(model)
    print("\tTraining time: %0.3fs" % train_time)
    print("\tPrediction time: %0.3fs" % predict_time)
    print("\tExplained variance:", explained_variance_score(y_test, y_pred))
    print("\tMean absolute error:", mean_absolute_error(y_test, y_pred))
    print("\tR2 score:", r2_score(y_test, y_pred))
    print()

"""By the different kind of scores we can see that the Ridghe Regression is able to achieve a good score in a good time.

# **Training Phase 3**

In the last step we use the GridSearchCV to optimize the hyperparamters of the model and choose the right combination of hyperparamters.

# **Ridge Regression**
"""

from sklearn.linear_model import Ridge
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import GridSearchCV

#define model
model = Ridge()

#define evaluation
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)

#define evaluation
cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)

Ridge(alpha=1.0,fit_intercept=True,copy_X=True,max_iter=None,tol=0.001,solver='auto',random_state=None,)

#define parameters
param = {
    'solver':['svd', 'cholesky', 'lsqr', 'sag'],
    'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100],
     'fit_intercept':[True, False]
}

#define search
search = GridSearchCV(model, param, scoring='neg_mean_absolute_error', n_jobs=-1, cv=cv)
#execute search
result = search.fit(X_train, y_train)

# # summarize result
print('Best Score: %s' % result.best_score_)
print('Best Hyperparameters: %s' % result.best_params_)

tuned_hyper_model = Ridge(alpha = 100, fit_intercept = False, solver = 'sag')

tuned_hyper_model.fit(X_train,y_train)
tuned_pred= tuned_hyper_model.predict(X_test)
tuned_score_test = tuned_hyper_model.score(X_test,y_test)
tuned_score_train = tuned_hyper_model.score(X_train,y_train)

# print(tuned_pred)
# print('tuned_score = ' , tuned_score_test ,tuned_score_train )
print('X_train = ' , X_train )

# #Error rate with hyperparameter tuning 
from sklearn import metrics

print( '\tMSE : ' , metrics.mean_squared_error(y_test,tuned_pred))
print( '\tMAE : ' , metrics.mean_absolute_error(y_test,tuned_pred))
print( '\tRMAE : ' ,np.sqrt(metrics.mean_squared_error(y_test,tuned_pred)))
print("\tExplained variance:", explained_variance_score(y_test, tuned_pred))
print("\tR2 score:", r2_score(y_test, tuned_pred))

d = {'scores': tuned_pred, 'original': y_test}
scoreTable = pd.DataFrame(data=d)
scoreTable

#from google.colab import drive
#drive.mount('/content/drive')

"""We use the pickle module to save our model and reuse it in the frontend."""

# save the model to disk
import pickle
filename = 'RidgeRegression.pkl'
pickle.dump(tuned_hyper_model, open(filename, 'wb'))
 
# some time later...
 
# load the model from disk
loaded_model = pickle.load(open(filename, 'rb'))
result = tuned_hyper_model.score(X_test, y_test)
print(result)

"""# **Credit-Calculation**
Despite our Machine Learning model, we need the difficutly scores based on the credits of the courses of the cogntive- and media science study programm by using Mean Normalizationn.

"""

# courses = pd.read_excel("/content/sample_data/data la.xlsx")
# courses

# sns.histplot(data=courses)

"""**Calculation of the Scores**"""

# n_ects = dict()

# for x in courses["ECTS"]:
#   mean = np.mean(courses["ECTS"])
#   z = (x - mean)/45
#   n_ects[str(x)] = z
# print(n_ects)

# series_ects = pd.Series(n_ects)
# series_ects.sort_values()

# final_score = 80 - (80 * n_ects["6"])
# print(final_score)

import pickle

rr = pickle.load(open('RidgeRegression.pkl', 'rb'))

X_train.columns

df = pd.DataFrame(
    
        {
  "10th Mark": [85],
  "12th Mark": [74],
  "prefer to study in_Night": [0],
    "Financial Status": [3],
    "daily studing time": [1],
  "willingness to pursue a career based on their degree  ": [4],
  "Stress Level ": [1],
    "Travelling Time ":[7],
  "social medai & video": [1],
  "Gender_Female": [1],
  "hobbies_Cinema": [1],
  "prefer to study in_Anytime": [1],
    "prefer to study in_Morning": [0],
  "hobbies_Sports": [0],
    "Department_B.com ISM": [0],
}
)
df
# print(df)
rr.predict(df)

